This document summarizes our Week 4 exploration with ChatGPT and Gemini as a team while developing our Recursive Language Model (RLM) project for STAT 4380. It reflects the discussions we had together with LLMs, the design decisions we made, the paths we tried, the things that failed, and the changes we made as we refined the scope toward a workable Week 4 deliverable.

At the beginning of the week, we explored multiple project directions based on the professor’s list: reinforcement learning environments, recursive language models, small-model fine-tuning, evolutionary strategies, and zeroth-order optimization. Through back-and-forth discussions with ChatGPT, we initially considered several directions, including evolutionary strategies and a full RL environment for editing Python code. However, based on the professor’s explicit guidance that evolutionary strategies would be slow and difficult to implement in Week 4, our AI conversations helped us converge quickly toward recursive editing as the central technique.

Working with ChatGPT helped us understand the core idea behind RLMs: the model does not need to hold the entire context inside its attention window. Instead, the model interacts with a Python REPL that stores the long context externally (as a variable like CONTEXT) and issues Python actions to search, filter, or transform it. This tool-use step lets the model “think” in multiple steps and access arbitrarily long information. This approach matches both the RLM-minimal repository and the Prime Intellect blog post describing RLMs. After several explorations, our team aligned on recreating a long-context retrieval task: hide a needle like “NEEDLE=value” inside a very long random text, store it externally, and let the agent recover the value using Python code inside the REPL.

A major goal of this week was to translate that idea into a working notebook. With ChatGPT’s help, we constructed a clear plan: a safe Python execution function with restricted builtins and timeouts; a generator that produces synthetic long-context data with an embedded key; an environment that initializes external memory; and a simple baseline agent that issues regex search code to find the value. This ensures that our notebook always runs even without a local LLM. We also created the structure for running batches of episodes, logging results, computing accuracy, tracking REPL steps, and printing a transcript we can paste directly into our Week 4 report.

As a team, we tried several more ambitious ideas before narrowing the scope. One attempt involved building a full RL environment where the agent edits multi-line Python code step-by-step until it produces a target output. This was too complex for Week 4; handling syntax errors, reward sparsity, and multi-step credit assignment created more overhead than expected. We also evaluated evolutionary strategies but realized that sampling many variations per iteration would be too slow and misaligned with the professor’s instruction to focus on recursive editing. We briefly experimented with prompting an LLM to generate REPL actions directly, but without a stable baseline environment, debugging was confusing, so we postponed that until later weeks.

Along the way, we encountered failure modes that helped refine our approach. The full RL code-editing plan collapsed under complexity. Evolutionary strategies needed too much sampling. Early attempts to integrate model-generated code failed due to syntax issues and missing guardrails. We also initially got stuck trying to finalize the full repository structure too early, but ChatGPT helped us realize the notebook should come first, with repo scaffolding added afterwards.

Based on this exploration, we made several important changes. First, we narrowed the project scope to reproducing an RLM-style long-context retrieval system as our Week 4 milestone. Second, we introduced a deterministic “tool agent” to guarantee that the notebook runs even if we cannot integrate an LLM yet. Third, we built a safe_exec utility to handle Python execution safely. Fourth, we added batch evaluation to produce metrics for the report. Fifth, we added the ability to show a complete REPL transcript so we can include a concrete example in our deliverables.

By the end of Week 4 exploration, our team has a clear understanding of recursive editing and RLMs. We now have a working external-memory environment, a functioning REPL sandbox, a deterministic agent baseline, and batch-level metrics (accuracy, steps, runtime). These components give us everything we need for the Week 4 report and notebook. We also have a realistic plan for next steps: integrate a real LLM to generate REPL actions, reproduce parts of the RLM-minimal training pipeline, add multi-turn reasoning, and eventually experiment with training or fine-tuning techniques using real rewards.

For Week 5 and beyond, we plan to integrate a small HuggingFace model to generate Python tool-use actions, reproduce the actual RLM-minimal inference and training flow, and scale the tasks from simple needle retrieval to problems that require multiple recursive steps. This will set the stage for meaningful experimentation later in the semester.
